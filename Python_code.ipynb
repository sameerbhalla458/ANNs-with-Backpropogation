{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# import requests\n",
    "# import gzip\n",
    "matplotlib.rc('xtick', labelsize=10) \n",
    "matplotlib.rc('ytick', labelsize=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing Data in .gz format (Run only one time for downloading)\n",
    "# # urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "# #         'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "# #         'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "# #         'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "# # res = requests.get(urls[3])\n",
    "# # with open('test_labels.gz','wb') as file:\n",
    "# # file.write(res.content)\n",
    "# # Converting .gz format to an array\n",
    "# with gzip.open('/Users/sameerbhalla/Library/CloudStorage/OneDrive-UniversityofCincinnati/Courses/Intelligent Systems/Assignments/Assignment 3/train_images.gz','rb') as file:\n",
    "#     Train_Images_Data = np.frombuffer(file.read(),dtype=np.int8,offset=16) # 28 x 28 x 60000\n",
    "#     Train_Images_Data = Train_Images_Data/255 # Data Scaling\n",
    "#     Train_Images_Data = Train_Images_Data.reshape(60000,784)\n",
    "# with gzip.open('/Users/sameerbhalla/Library/CloudStorage/OneDrive-UniversityofCincinnati/Courses/Intelligent Systems/Assignments/Assignment 3/train_labels.gz','rb') as file:\n",
    "#     Train_Labels_Data = np.frombuffer(file.read(),dtype=np.int8,offset=8) # 60000\n",
    "#     Train_Labels_Data = Train_Labels_Data.reshape(60000,1)\n",
    "# with gzip.open('/Users/sameerbhalla/Library/CloudStorage/OneDrive-UniversityofCincinnati/Courses/Intelligent Systems/Assignments/Assignment 3/test_images.gz','rb') as file:\n",
    "#     Test_Images_Data = np.frombuffer(file.read(),dtype=np.int8,offset=16) # 28 x 28 x 10000\n",
    "#     Test_Images_Data = Test_Images_Data/255\n",
    "#     Test_Images_Data = Test_Images_Data.reshape(10000,784)\n",
    "# with gzip.open('/Users/sameerbhalla/Library/CloudStorage/OneDrive-UniversityofCincinnati/Courses/Intelligent Systems/Assignments/Assignment 3/test_labels.gz','rb') as file:\n",
    "#     Test_Labels_Data = np.frombuffer(file.read(),dtype=np.int8,offset=8) # 10000\n",
    "#     Test_Labels_Data = Test_Labels_Data.reshape(10000,1)\n",
    "# # Verification of Images and Labels\n",
    "# j=7777\n",
    "# print(Train_Labels_Data[j])\n",
    "# Image = Train_Images_Data[j].reshape(28,28)\n",
    "# fig = plt.figure(figsize=(6,6))\n",
    "# ax = fig.add_subplot()\n",
    "# ax.imshow(Image)\n",
    "# # Concatenate Bias(1) and Inputs\n",
    "# Train_Images_Data = np.concatenate( ( np.ones((60000,1)) , Train_Images_Data ) , axis=1 )#Array of 60000 x 785\n",
    "# Test_Images_Data = np.concatenate( ( np.ones((10000,1)) , Test_Images_Data ) , axis=1 )#Array of 10000 x 785\n",
    "# Tr_Img = Train_Images_Data\n",
    "# Ts_Img = Test_Images_Data\n",
    "# Tr_Lbl = Train_Labels_Data\n",
    "# Ts_Lbl = Test_Labels_Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Text Data and Converting to an Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_data = np.loadtxt(\"MNISTnumImages5000_balanced.txt\") #Array of 5000 x 784\n",
    "Labels_data = np.loadtxt(\"MNISTnumLabels5000_balanced.txt\", dtype ='int') #Array of 5000 x 1\n",
    "Total_data = np.concatenate( ( Images_data , Labels_data.reshape(5000,1) ) , axis=1 )#Array of 5000 x 785\n",
    "SL = np.size(Images_data,1) #Sample Length = 784\n",
    "SS = np.size(Total_data,0) #Sample Size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the Arrays into Training & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating data of 5000 samples into 10 sets\n",
    "set_0,set_1,set_2,set_3,set_4,set_5,set_6,set_7,set_8,set_9 = ( np.array([]) for i in range (10) )\n",
    "for i in range(0,SS):\n",
    "    if Total_data[i,SL] == 0:#784th\n",
    "        set_0 = np.append(set_0,Total_data[i])\n",
    "    if Total_data[i,SL] == 1:\n",
    "        set_1 = np.append(set_1,Total_data[i])\n",
    "    if Total_data[i,SL] == 2:\n",
    "        set_2 = np.append(set_2,Total_data[i])\n",
    "    if Total_data[i,SL] == 3:\n",
    "        set_3 = np.append(set_3,Total_data[i])\n",
    "    if Total_data[i,SL] == 4:\n",
    "        set_4 = np.append(set_4,Total_data[i])\n",
    "    if Total_data[i,SL] == 5:\n",
    "        set_5 = np.append(set_5,Total_data[i])\n",
    "    if Total_data[i,SL] == 6:\n",
    "        set_6 = np.append(set_6,Total_data[i])\n",
    "    if Total_data[i,SL] == 7:\n",
    "        set_7 = np.append(set_7,Total_data[i])\n",
    "    if Total_data[i,SL] == 8:\n",
    "        set_8 = np.append(set_8,Total_data[i])\n",
    "    if Total_data[i,SL] == 9:\n",
    "        set_9 = np.append(set_9,Total_data[i])\n",
    "\n",
    "set_0 = np.reshape(set_0,(int(SS/10),SL+1))\n",
    "set_1 = np.reshape(set_1,(int(SS/10),SL+1))\n",
    "set_2 = np.reshape(set_2,(int(SS/10),SL+1))\n",
    "set_3 = np.reshape(set_3,(int(SS/10),SL+1))\n",
    "set_4 = np.reshape(set_4,(int(SS/10),SL+1))\n",
    "set_5 = np.reshape(set_5,(int(SS/10),SL+1))\n",
    "set_6 = np.reshape(set_6,(int(SS/10),SL+1))\n",
    "set_7 = np.reshape(set_7,(int(SS/10),SL+1))\n",
    "set_8 = np.reshape(set_8,(int(SS/10),SL+1))\n",
    "set_9 = np.reshape(set_9,(int(SS/10),SL+1))\n",
    "\n",
    "Training_set = np.concatenate( ( set_0[range(0,400)] , set_1[range(0,400)] , set_2[range(0,400)] , set_3[range(0,400)] , set_4[range(0,400)] , set_5[range(0,400)] , set_6[range(0,400)] , set_7[range(0,400)] , set_8[range(0,400)] , set_9[range(0,400)]  ) , axis=0)\n",
    "Test_set = np.concatenate( ( set_0[range(400,500)] , set_1[range(400,500)] , set_2[range(400,500)] , set_3[range(400,500)] , set_4[range(400,500)] , set_5[range(400,500)] , set_6[range(400,500)] , set_7[range(400,500)] , set_8[range(400,500)] , set_9[range(400,500)]  ) , axis=0)\n",
    "random.shuffle(Training_set)\n",
    "random.shuffle(Test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the number of Hidden layers and Hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = SL  #Number of Input neurons = 784\n",
    "Hid = 1 #Number of hidden layers\n",
    "m = np.array([200]) #Number of hidden neurons ; Change according to Hid\n",
    "l = np.array([10]) #Number of output neurons\n",
    "N_set = m\n",
    "N_set = np.append(N_set,l)\n",
    "N_set = np.insert(N_set,0,n) #Number of Neurons in each set ; [784,150,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(np.size(N_set,0)-1):\n",
    "    globals()[f\"W{f+1}\"] = np.random.normal(scale=0.5, size=(N_set[f], N_set[f+1]))#W1 = 784x200 , W2 = 200x10, W3 - - -\n",
    "    globals()[f\"Ch{f+1}\"] = np.zeros((N_set[f], N_set[f+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of Error Fractions for Training and Test set before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.25\n",
    "H = 0.75\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "EF_Tr_in = 0 #in = initial\n",
    "EF_Ts_in = 0\n",
    "for g in range(np.size(Training_set,0)):#0 to 3999 , Training Set\n",
    "    Active_set = Training_set[g,range(0,SL)] #Active Set\n",
    "    Active_set_Z = Training_set[g,SL] # Correct Value\n",
    "    A0 = Active_set\n",
    "    for s in range(np.size(N_set,0)-1):#s in 0,1,2\n",
    "        globals()[f\"Z{s+1}\"] = np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"])\n",
    "        globals()[f\"A{s+1}\"] = sigmoid(globals()[f\"Z{s+1}\"])\n",
    "    if np.max(globals()[f\"A{s+1}\"], axis=0) >= H:\n",
    "        Active_set_Z_calc = np.argmax(globals()[f\"A{s+1}\"], axis=0)\n",
    "        Active_set_Z_err = Active_set_Z - Active_set_Z_calc\n",
    "        if Active_set_Z_err == 0:\n",
    "            count1 = count1 + 1\n",
    "EF_Tr_in = (np.size(Training_set,0) - count1)/np.size(Training_set,0)\n",
    "\n",
    "for h in range(np.size(Test_set,0)):#0 to 999 , Test Set\n",
    "    Active_set = Test_set[h,range(0,SL)] #Active Set\n",
    "    Active_set_Z = Test_set[h,SL] # Correct Value\n",
    "    A0 = Active_set\n",
    "    for s in range(np.size(N_set,0)-1):#s in 0,1,2\n",
    "        globals()[f\"Z{s+1}\"] = np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"])\n",
    "        globals()[f\"A{s+1}\"] = sigmoid(globals()[f\"Z{s+1}\"])\n",
    "    if np.max(globals()[f\"A{s+1}\"], axis=0) >= H:\n",
    "        Active_set_Z_calc = np.argmax(globals()[f\"A{s+1}\"], axis=0)\n",
    "        Active_set_Z_err = Active_set_Z - Active_set_Z_calc\n",
    "        if Active_set_Z_err == 0:\n",
    "            count2 = count2 + 1\n",
    "EF_Ts_in = (np.size(Test_set,0) - count2)/np.size(Test_set,0)\n",
    "\n",
    "# print(count1,EF_Tr_in,count2,EF_Ts_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Error Fraction Calculation of Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 #Number of Epochs\n",
    "eta = 0.03 #Learning Rate\n",
    "alpha = 0.2 #Momentum\n",
    "subset_Tr_size = 4000\n",
    "subset_Ts_size = 1000\n",
    "EF_Tr_set = np.array([EF_Tr_in])\n",
    "EF_Ts_set = np.array([EF_Ts_in])\n",
    "for i in range(epochs):#0 to epochs\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "\n",
    "    #Training Set\n",
    "    rand_row_Tr = np.random.randint(np.size(Training_set,0), size=subset_Tr_size)\n",
    "    Training_subset = Training_set[rand_row_Tr]#Stochastic gradient descent; 2000 x 785\n",
    "    for j in range(np.size(Training_subset,0)):# j in 0 to 3999\n",
    "        Z_set = np.zeros(l)\n",
    "        Active_set = Training_subset[j,range(0,SL)] #Active Set\n",
    "        Active_set_Z = Training_subset[j,SL] # Correct Value\n",
    "        Z_set[int(Active_set_Z)] = 1\n",
    "\n",
    "        #Forward Propagation\n",
    "        A0 = Active_set\n",
    "        for s in range(np.size(N_set,0)-1):#s in 0,1,2\n",
    "            globals()[f\"Z{s+1}\"] = np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"])\n",
    "            globals()[f\"A{s+1}\"] = sigmoid(globals()[f\"Z{s+1}\"])\n",
    "\n",
    "        #Back Propagation (Calculating Deltas)\n",
    "        globals()[f\"D{s+1}\"] = ( Z_set - globals()[f\"A{s+1}\"]) * derivative(globals()[f\"Z{s+1}\"])\n",
    "        globals()[f\"Ch{s+1}\"] = eta * np.outer( globals()[f\"A{s}\"] , globals()[f\"D{s+1}\"] ) - alpha * globals()[f\"Ch{s+1}\"]#Outer Layer - 3\n",
    "        for k in reversed(range(1,np.size(N_set,0)-1)):#Hidden Layers ; #s in 2,1\n",
    "            globals()[f\"D{k}\"] = np.dot( globals()[f\"W{k+1}\"] , globals()[f\"D{k+1}\"] ) * derivative(globals()[f\"Z{k}\"])\n",
    "            globals()[f\"Ch{k}\"] = eta * np.outer( globals()[f\"A{k-1}\"] , globals()[f\"D{k}\"] ) - alpha * globals()[f\"Ch{k}\"]\n",
    "\n",
    "        #Changing the Weights\n",
    "        for t in range(np.size(N_set,0)-1):#t in 0,1,2\n",
    "            globals()[f\"W{t+1}\"] = globals()[f\"W{t+1}\"] + globals()[f\"Ch{t+1}\"]\n",
    "            \n",
    "        #Calculating Error Fraction of Training Set at 10th epoch\n",
    "        if i % 10 == 0:\n",
    "            if np.max(sigmoid( np.dot(globals()[f\"A{t}\"],globals()[f\"W{t+1}\"]) ), axis=0) >= H:\n",
    "                Active_set_Z_calc = np.argmax( sigmoid( np.dot(globals()[f\"A{t}\"],globals()[f\"W{t+1}\"]) ) , axis=0)\n",
    "                Active_set_Z_err = Active_set_Z - Active_set_Z_calc\n",
    "                if Active_set_Z_err == 0:\n",
    "                    count3 = count3 + 1\n",
    "\n",
    "    #Test Set\n",
    "    rand_row_Ts = np.random.randint(np.size(Test_set,0), size=subset_Ts_size)\n",
    "    Test_subset = Test_set[rand_row_Ts]#Stochastic gradient descent; 2000 x 785\n",
    "    for p in range(np.size(Test_subset,0)):# p in 0 to 999\n",
    "        Z_set = np.zeros(l)\n",
    "        Active_set = Test_subset[p,range(0,SL)] #Active Set\n",
    "        Active_set_Z = Test_subset[p,SL] # Correct Value\n",
    "        Z_set[int(Active_set_Z)] = 1\n",
    "\n",
    "        #Forward Propagation\n",
    "        A0 = Active_set\n",
    "        for s in range(np.size(N_set,0)-1):  #s in 0,1,2\n",
    "            globals()[f\"Z{s+1}\"] = np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"])\n",
    "            globals()[f\"A{s+1}\"] = sigmoid(globals()[f\"Z{s+1}\"])\n",
    "\n",
    "        #Calculating Error Fraction of Test Set\n",
    "        if i % 10 == 0:\n",
    "            if np.max(sigmoid( np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"]) ), axis=0) >= H:\n",
    "                Active_set_Z_calc = np.argmax( sigmoid( np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"]) ) , axis=0)\n",
    "                Active_set_Z_err = Active_set_Z - Active_set_Z_calc\n",
    "                if Active_set_Z_err == 0:\n",
    "                    count4 = count4 + 1\n",
    "    if i % 10 == 0:\n",
    "        EF_Tr_in = (np.size(Training_subset,0) - count3)/np.size(Training_subset,0)\n",
    "        EF_Ts_in = (np.size(Test_subset,0) - count4)/np.size(Test_subset,0)\n",
    "        EF_Tr_set = np.append(EF_Tr_set,EF_Tr_in)\n",
    "        EF_Ts_set = np.append(EF_Ts_set,EF_Ts_in)\n",
    "        print(i,count3,EF_Tr_in,count4,EF_Ts_in)\n",
    "# print(EF_Tr_set)\n",
    "# print(EF_Ts_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of Error Fraction of Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15,5))\n",
    "ax1 = fig1.add_subplot()\n",
    "l1=ax1.plot(range(-10,10*(np.size(EF_Tr_set)-1),10),EF_Tr_set)\n",
    "l2=ax1.plot(range(-10,10*(np.size(EF_Ts_set)-1),10),EF_Ts_set)\n",
    "ax1.set_xlabel('Epochs',fontsize=10)\n",
    "ax1.set_ylabel('Error Fraction',fontsize=10)\n",
    "labels = [\"Training Set\", \"Test Set\"]\n",
    "fig1.legend([l1, l2], labels=labels,prop={'size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random20 = np.array([130, 142, 131, 106, 109, 26, 192, 70, 96, 23, 188, 107, 98, 151, 32, 158, 181, 153, 11, 33])\n",
    "print(random20)\n",
    "random20W = globals()[f\"W{t}\"][:,random20]\n",
    "\n",
    "fig4 = plt.figure(figsize=(10,8))\n",
    "for x in range(20):\n",
    "    globals()[f\"Image{x+1}\"] = np.transpose(random20W[range(0,784),x].reshape(28,28))\n",
    "\n",
    "ax4 = fig4.add_subplot(4,5,1)\n",
    "ax4.imshow(Image1)\n",
    "ax4 = fig4.add_subplot(4,5,2)\n",
    "ax4.imshow(Image2)\n",
    "ax4 = fig4.add_subplot(4,5,3)\n",
    "ax4.imshow(Image3)\n",
    "ax4 = fig4.add_subplot(4,5,4)\n",
    "ax4.imshow(Image4)\n",
    "ax4 = fig4.add_subplot(4,5,5)\n",
    "ax4.imshow(Image5)\n",
    "ax4 = fig4.add_subplot(4,5,6)\n",
    "ax4.imshow(Image6)\n",
    "ax4 = fig4.add_subplot(4,5,7)\n",
    "ax4.imshow(Image7)\n",
    "ax4 = fig4.add_subplot(4,5,8)\n",
    "ax4.imshow(Image8)\n",
    "ax4 = fig4.add_subplot(4,5,9)\n",
    "ax4.imshow(Image9)\n",
    "ax4 = fig4.add_subplot(4,5,10)\n",
    "ax4.imshow(Image10)\n",
    "ax4 = fig4.add_subplot(4,5,11)\n",
    "ax4.imshow(Image11)\n",
    "ax4 = fig4.add_subplot(4,5,12)\n",
    "ax4.imshow(Image12)\n",
    "ax4 = fig4.add_subplot(4,5,13)\n",
    "ax4.imshow(Image13)\n",
    "ax4 = fig4.add_subplot(4,5,14)\n",
    "ax4.imshow(Image14)\n",
    "ax4 = fig4.add_subplot(4,5,15)\n",
    "ax4.imshow(Image15)\n",
    "ax4 = fig4.add_subplot(4,5,16)\n",
    "ax4.imshow(Image16)\n",
    "ax4 = fig4.add_subplot(4,5,17)\n",
    "ax4.imshow(Image17)\n",
    "ax4 = fig4.add_subplot(4,5,18)\n",
    "ax4.imshow(Image18)\n",
    "ax4 = fig4.add_subplot(4,5,19)\n",
    "ax4.imshow(Image19)\n",
    "ax4 = fig4.add_subplot(4,5,20)\n",
    "ax4.imshow(Image20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conf_Mat_Tr = np.zeros((10, 10))\n",
    "Conf_Mat_Ts = np.zeros((10, 10))\n",
    "\n",
    "for g in range(np.size(Training_set,0)):#0 to 3999 , Training Set\n",
    "    Active_set = Training_set[g,range(0,SL)] #Active Set\n",
    "    Active_set_Z = Training_set[g,SL] # Correct Value\n",
    "    A0 = Active_set\n",
    "    for s in range(np.size(N_set,0)-1):#s in 0,1,2\n",
    "        globals()[f\"Z{s+1}\"] = np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"])\n",
    "        globals()[f\"A{s+1}\"] = sigmoid(globals()[f\"Z{s+1}\"])\n",
    "    if np.max(globals()[f\"A{s+1}\"], axis=0) >= H:\n",
    "        Active_set_Z_calc = np.argmax(globals()[f\"A{s+1}\"], axis=0)\n",
    "    Conf_Mat_Tr[int(Active_set_Z),Active_set_Z_calc] = Conf_Mat_Tr[int(Active_set_Z),Active_set_Z_calc] + 1\n",
    "print(Conf_Mat_Tr)\n",
    "\n",
    "for g in range(np.size(Test_set,0)):#0 to 999 , Test Set\n",
    "    Active_set = Test_set[g,range(0,SL)] #Active Set\n",
    "    Active_set_Z = Test_set[g,SL] # Correct Value\n",
    "    A0 = Active_set\n",
    "    for s in range(np.size(N_set,0)-1):#s in 0,1,2\n",
    "        globals()[f\"Z{s+1}\"] = np.dot(globals()[f\"A{s}\"],globals()[f\"W{s+1}\"])\n",
    "        globals()[f\"A{s+1}\"] = sigmoid(globals()[f\"Z{s+1}\"])\n",
    "    if np.max(globals()[f\"A{s+1}\"], axis=0) >= H:\n",
    "        Active_set_Z_calc = np.argmax(globals()[f\"A{s+1}\"], axis=0)\n",
    "    Conf_Mat_Ts[int(Active_set_Z),Active_set_Z_calc] = Conf_Mat_Ts[int(Active_set_Z),Active_set_Z_calc] + 1\n",
    "print(Conf_Mat_Ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
